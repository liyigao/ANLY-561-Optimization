{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Yigao Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Prove that $\\nabla(f\\circ g)(x,y)=Dg(x,y)^T\\nabla f(g(x,y))$  \n",
    "$$\\begin{aligned}\n",
    "LHS=\\nabla(f\\circ g)(x,y)&=\\begin{pmatrix}\\frac{\\partial(f\\circ g)}{\\partial x}(x,y)\\\\ \\frac{\\partial(f\\circ g)}{\\partial y}(x,y)\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\\begin{pmatrix}\\frac{\\partial f}{\\partial g_1}(x,y) && \\frac{\\partial f}{\\partial g_2}(x,y)\\end{pmatrix}\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x}(x,y) \\\\ \\frac{\\partial g_2}{\\partial x}(x,y)\\end{pmatrix} \\\\ \\begin{pmatrix}\\frac{\\partial f}{\\partial g_1}(x,y) && \\frac{\\partial f}{\\partial g_2}(x,y)\\end{pmatrix}\\begin{pmatrix}\\frac{\\partial g_1}{\\partial y}(x,y) \\\\ \\frac{\\partial g_2}{\\partial y}(x,y)\\end{pmatrix} \\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\\frac{\\partial f}{\\partial g_1}(x,y)\\frac{\\partial g_1}{\\partial x}(x,y)+\\frac{\\partial f}{\\partial g_2}(x,y)\\frac{\\partial g_2}{\\partial x}(x,y) \\\\ \\frac{\\partial f}{\\partial g_1}(x,y)\\frac{\\partial g_1}{\\partial y}(x,y)+\\frac{\\partial f}{\\partial g_2}(x,y)\\frac{\\partial g_2}{\\partial y}(x,y) \\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x} && \\frac{\\partial g_2}{\\partial x} \\\\ \\frac{\\partial g_1}{\\partial y} && \\frac{\\partial g_2}{\\partial y}\\end{pmatrix}\\begin{pmatrix}\\frac{\\partial f}{\\partial g_1} \\\\ \\frac{\\partial f}{\\partial g_2}\\end{pmatrix}\\\\\n",
    "&=Dg(x,y)^T\\nabla f(g(x,y))=RHS\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "(a) $\\ell_{lin}(\\beta_0,\\beta_1)=\\frac{1}{10}\\sum_{i=1}^{10}(y_i-\\beta_1x_i-\\beta_0)^2$\n",
    "$$\\frac{\\partial\\ell_{lin}}{\\partial \\beta_0}=-\\frac15\\sum_{i=1}^{10}(y_i-\\beta_1x_i-\\beta_0)$$  \n",
    "$$\\frac{\\partial\\ell_{lin}}{\\partial \\beta_1}=-\\frac15\\sum_{i=1}^{10}x_i(y_i-\\beta_1x_i-\\beta_0)$$  \n",
    "$$\\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_0^2}=\\frac15\\times10=2$$  \n",
    "$$\\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_1^2}=\\frac{\\sum_{i=1}^{10}x_i^2}{5}$$  \n",
    "$$\\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_0\\partial\\beta_1}=\\frac{\\sum_{i=1}^{10}x_i}{5}$$  \n",
    "$$\\begin{aligned}\n",
    "\\nabla^2\\ell_{lin}(\\beta_0,\\beta_1)&=\\begin{pmatrix}\\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_0^2} && \\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_0\\partial\\beta_1} \\\\ \\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_0\\partial\\beta_1} && \\frac{\\partial^2\\ell_{lin}}{\\partial\\beta_1^2}\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}2 && \\frac15\\sum_{i=1}^{10}x_i \\\\ \\frac15\\sum_{i=1}^{10}x_i && \\frac15\\sum_{i=1}^{10}x_i^2\\end{pmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "Since $2>0$ and, by plugging in data, det$(\\nabla^2\\ell_{lin}(\\beta_0,\\beta_1))=\\frac{69}{25}>0$, matrix $\\nabla^2\\ell_{lin}(\\beta_0,\\beta_1)$ is positive definite. Therefore, $\\ell_{lin}$ is strictly convex. To find the minimizer:  \n",
    "$$\\begin{cases}\n",
    "\\frac{\\partial\\ell_{lin}}{\\partial \\beta_0}=2\\beta_0+\\frac15\\beta_1=0\\\\\n",
    "\\frac{\\partial\\ell_{lin}}{\\partial \\beta_1}=\\frac15\\beta_0+\\frac75\\beta_1-1=0\\\\\n",
    "\\end{cases}\\implies\\begin{cases}\n",
    "\\beta_0=-\\frac5{69}\\\\\n",
    "\\beta_1=\\frac{50}{69}\\\\\n",
    "\\end{cases}$$\n",
    "(b) $\\ell_{log}(\\beta_0,\\beta_1)=\\frac{1}{10}\\sum_{i=1}^{10}log(1+e^{-y_i(\\beta_1x_i+\\beta_0)})$\n",
    "$$\\frac{\\partial\\ell_{log}}{\\partial \\beta_0}=\\frac1{10}\\sum_{i=1}^{10}\\frac{-y_ie^{-y_i(\\beta_1x_i+\\beta_0)}}{1+e^{-y_i(\\beta_1x_i+\\beta_0)}}$$\n",
    "$$\\frac{\\partial\\ell_{log}}{\\partial \\beta_1}=\\frac1{10}\\sum_{i=1}^{10}\\frac{-x_iy_ie^{-y_i(\\beta_1x_i+\\beta_0)}}{1+e^{-y_i(\\beta_1x_i+\\beta_0)}}$$\n",
    "$$\\frac{\\partial^2\\ell_{log}}{\\partial\\beta_0^2}=\\frac1{10}\\sum_{i=1}^{10}\\frac{y_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}}$$\n",
    "$$\\frac{\\partial^2\\ell_{log}}{\\partial\\beta_1^2}=\\frac1{10}\\sum_{i=1}^{10}\\frac{x_i^2y_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}}$$\n",
    "$$\\frac{\\partial^2\\ell_{log}}{\\partial\\beta_0\\partial\\beta_1}=\\frac1{10}\\sum_{i=1}^{10}\\frac{x_iy_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}}$$\n",
    "$$\\begin{aligned}\n",
    "\\nabla^2\\ell_{log}(\\beta_0,\\beta_1)&=\\begin{pmatrix}\\frac{\\partial^2\\ell_{log}}{\\partial\\beta_0^2} && \\frac{\\partial^2\\ell_{log}}{\\partial\\beta_0\\partial\\beta_1} \\\\ \\frac{\\partial^2\\ell_{log}}{\\partial\\beta_0\\partial\\beta_1} && \\frac{\\partial^2\\ell_{log}}{\\partial\\beta_1^2}\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\\frac1{10}\\sum_{i=1}^{10}\\frac{y_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}} && \\frac1{10}\\sum_{i=1}^{10}\\frac{x_iy_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}} \\\\ \\frac1{10}\\sum_{i=1}^{10}\\frac{x_iy_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}} && \\frac1{10}\\sum_{i=1}^{10}\\frac{x_i^2y_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}}\\end{pmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "Since $\\frac1{10}\\sum_{i=1}^{10}\\frac{y_i^2e^{-y_i(\\beta_1x_i+\\beta_0)}}{(1+e^{-y_i(\\beta_1x_i+\\beta_0))^2}}>0$ is always true, and det$(\\nabla^2\\ell_{log}(\\beta_0,\\beta_1))>0$, matrix $\\nabla^2\\ell_{log}(\\beta_0,\\beta_1)$ is positive definite. Therefore, $\\ell_{log}$ is strictly convex. The necessary conditions for optimality of $(\\beta_0^*,\\beta_1^*)$: If $(\\beta_0^*,\\beta_1^*)$ is a minimizer of $\\ell_{log}$, then $\\nabla \\ell_{log}(\\beta_0^*,\\beta_1^*)={\\bf 0}$. The sufficient conditions for optimality of $(\\beta_0^*,\\beta_1^*)$: If $(\\beta_0^*,\\beta_1^*)\\in\\mathbb{R}^2$ satisfies $\\nabla \\ell_{log}(\\beta_0^*,\\beta_1^*)={\\bf 0}$, then $(\\beta_0^*,\\beta_1^*)$ is a minimizer of $\\ell_{log}$ on $\\mathbb{R}^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Let $A=\\begin{pmatrix}a&&b\\\\b&&c\\end{pmatrix}$ and ${\\bf x}^*=\\begin{pmatrix}x\\\\y\\end{pmatrix}$. $f(x,y)=f({\\bf x}^*)=-\\frac12{{\\bf x}^*}^TA{\\bf x}^*=-\\frac12(ax^2+2bxy+cy^2)$. $g(x,y)=g({\\bf x}^*)=||{\\bf x}||^2-1=x^2+y^2-1$.\n",
    "By Lagrange Multipliers, there exists $\\lambda\\in\\mathbb{R}$ such that $\\nabla f=\\lambda\\nabla g$ at minimizer.\n",
    "$$\\nabla f=\\begin{pmatrix}-ax-by\\\\-bx-cy\\end{pmatrix}$$  \n",
    "$$\\nabla g=\\begin{pmatrix}2x\\\\2y\\end{pmatrix}$$\n",
    "$$\\begin{cases}\n",
    "-ax-by=2\\lambda x\\\\\n",
    "-bx-cy=2\\lambda y\\\\\n",
    "x^2+y^2-1=0\\\\\n",
    "\\end{cases}$$\n",
    "$\\lambda=\\frac{-a-c\\pm\\sqrt{(a-c)^2+4b^2}}4$. Take $\\lambda=\\frac{-a-c-\\sqrt{(a-c)^2+4b^2}}4$.\n",
    "$$\\begin{cases}\n",
    "x^2=\\frac{2b^2}{(a-c)^2+4b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}\\\\\n",
    "y^2=\\frac{(a-c)^2+2b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}{(a-c)^2+4b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}\n",
    "\\end{cases}$$\n",
    "Regarding to eigenvalues and eigenvectors of $A$, its largest eigenvalue is $\\frac{a+c+\\sqrt{(a-c)^2+4b^2}}2$. By solving the equations $\\begin{cases}av_1+bv_2=\\frac{a+c+\\sqrt{(a-c)^2+4b^2}}2v1\\\\bv_1+cv_2=\\frac{a+c+\\sqrt{(a-c)^2+4b^2}}2v2\\end{cases}$. If $v_1^2=x^2=\\frac{2b^2}{(a-c)^2+4b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}$, then $v_2^2=\\frac{(a-c)^2+2b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}{(a-c)^2+4b^2+(c-a)\\sqrt{(a-c)^2+4b^2}}=y^2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
