{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Yigao Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1  \n",
    "(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Avg Cross Entropy: 0.687912, Gradient Norm: 0.407086, Training Accuracy: 56.8 percent\n",
      "Step: 1, Avg Cross Entropy: 0.684625, Gradient Norm: 0.049936, Training Accuracy: 56.8 percent\n",
      "Step: 2, Avg Cross Entropy: 0.684061, Gradient Norm: 0.034535, Training Accuracy: 56.8 percent\n",
      "Step: 3, Avg Cross Entropy: 0.684007, Gradient Norm: 0.010562, Training Accuracy: 56.8 percent\n",
      "Step: 4, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 5, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 6, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 7, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 8, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 9, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 10, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 11, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 12, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 13, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 14, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 15, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 16, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 17, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 18, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 19, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 20, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 21, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 22, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 23, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 24, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 25, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 26, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 27, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 28, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 29, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 30, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 31, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 32, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 33, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 34, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 35, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 36, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 37, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 38, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 39, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 40, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 41, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 42, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 43, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 44, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 45, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 46, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 47, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 48, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 49, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 50, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 51, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 52, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 53, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 54, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 55, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 56, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 57, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 58, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 59, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 60, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 61, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 62, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 63, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 64, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 65, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 66, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 67, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 68, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 69, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 70, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 71, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 72, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 73, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 74, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 75, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 76, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 77, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 78, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 79, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 80, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 81, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 82, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 83, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 84, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 85, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 86, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 87, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 88, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 89, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 90, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 91, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 92, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 93, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 94, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 95, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 96, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 97, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 98, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Step: 99, Avg Cross Entropy: 0.684007, Gradient Norm: 0.004906, Training Accuracy: 56.8 percent\n",
      "Final test accuracy: 76.9 percent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def chain_rule(Dg, Df, var_shape):\n",
    "    # Computes the Jacobian D (g o f)\n",
    "    dim = len(var_shape)\n",
    "    Dg_axes = list(range(Dg.ndim-dim, Dg.ndim))\n",
    "    Df_axes = list(range(dim))\n",
    "    return np.tensordot(Dg, Df, axes=(Dg_axes, Df_axes))\n",
    "\n",
    "# Compute the Jacobian blocks of X @ W + b\n",
    "\n",
    "def DX_affine(X, W, b):\n",
    "    # (d_{x_{i, j}} (X @ W))_{a, b} = e_a^T e_ie_j^T W e_b, so a,i slices equal W.T\n",
    "    D = np.zeros((X.shape[0], W.shape[1], X.shape[0], X.shape[1]))\n",
    "    for k in range(X.shape[0]):\n",
    "        D[k,:,k,:]=W.T\n",
    "    return D, X.shape\n",
    "\n",
    "def DW_affine(X, W, b):\n",
    "    # (d_{w_{i, j}} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b, so b, j slices equal x\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k]=X\n",
    "    return D, W.shape\n",
    "\n",
    "def Db_affine(X, W, b):\n",
    "    # (d_{b_i} (1 @ b))_{a, b} = e_a^T 1 e_i^T e_b, so b, i slices are all ones\n",
    "    D = np.zeros((X.shape[0], W.shape[1], b.shape[1]))\n",
    "    for k in range(b.shape[1]):\n",
    "        D[:,k,k]=1\n",
    "    return D, b.shape\n",
    "    \n",
    "def logit(z):\n",
    "    # This is vectorized\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    A = logit(Z) * logit(-Z)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            D[i,j,i,j] = A[i,j]\n",
    "    return D, Z.shape\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - np.outer(v,v)\n",
    "        #print(D[k,:,k,:])\n",
    "    return D, Z.shape\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q))/P.shape[0]\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P * (1/Q)/P.shape[0], Q.shape\n",
    "\n",
    "def nn_loss_closure(X, Y):\n",
    "    # vars[0]=W_1, vars[1]=b_1, vars[2]=W_2, vars[3]=b_2\n",
    "    # cross_entropy(Y, matrix_softmax(affine(logit(affine(X; W_1, b_1))); W_2, b_2))\n",
    "    def f(var):\n",
    "        return cross_entropy(Y, matrix_softmax(logit((logit((X @ var[0]) + var[1]) @ var[2]) + var[3]) @ var[4] + var[5]))\n",
    "    return f\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(var):\n",
    "        # Activation of first layer\n",
    "        Z1 = (X @ var[0]) + var[1]\n",
    "        X2 = logit(Z1)\n",
    "        \n",
    "        # Activation of second layer\n",
    "        Z2 = (X2 @ var[2]) + var[3]\n",
    "        X3 = matrix_softmax(Z2)\n",
    "        \n",
    "        Z3 = (X3 @ var[4]) + var[5]\n",
    "        Q = matrix_softmax(Z3)\n",
    "\n",
    "        D_Q, Qshape = DQcross_entropy(Y, Q)\n",
    "        D_Z3, Z3shape = Dmatrix_softmax(Z3)\n",
    "        back_prop3 = chain_rule(D_Q, D_Z3, Qshape)\n",
    "\n",
    "        D_X3, X3shape = DX_affine(X3, var[4], var[5])\n",
    "        D_W3, W3shape = DW_affine(X3, var[4], var[5])\n",
    "        D_b3, b3shape = Db_affine(X3, var[4], var[5])\n",
    "        \n",
    "        D_Z2, Z2shape = Dlogit(Z2)\n",
    "        back_prop2 = chain_rule(chain_rule(back_prop3, D_X3, X3shape), D_Z2, Z2shape)\n",
    "        \n",
    "        D_X2, X2shape = DX_affine(X2, var[2], var[3])\n",
    "        D_W2, W2shape = DW_affine(X2, var[2], var[3])\n",
    "        D_b2, b2shape = Db_affine(X2, var[2], var[3]) \n",
    "        \n",
    "        D_Z1, Z1shape = Dlogit(Z1)\n",
    "        back_prop1 = chain_rule(chain_rule(back_prop2, D_X2, X2shape), D_Z1, Z1shape)\n",
    "\n",
    "        # Jacobians for phi_1\n",
    "        D_W1, W1shape = DW_affine(X, var[0], var[1])\n",
    "        D_b1, b1shape = Db_affine(X, var[0], var[1])\n",
    "        \n",
    "        # Compute all the gradients\n",
    "        W1grad = chain_rule(back_prop1, D_W1, W1shape)\n",
    "        b1grad = chain_rule(back_prop1, D_b1, b1shape)\n",
    "        W2grad = chain_rule(back_prop2, D_W2, W2shape)\n",
    "        b2grad = chain_rule(back_prop2, D_b2, b2shape)\n",
    "        W3grad = chain_rule(back_prop3, D_W3, W3shape)\n",
    "        b3grad = chain_rule(back_prop3, D_b3, b3shape)\n",
    "        \n",
    "        return [W1grad, b1grad, W2grad, b2grad, W3grad, b3grad]\n",
    "    return df\n",
    "\n",
    "def update_blocks(x,y,t):\n",
    "    # An auxiliary function for backtracking with blocks of variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = x[i] + t*y[i]\n",
    "    return z\n",
    "                           \n",
    "def block_backtracking(x0, f, dx, df0, alpha=0.1, beta=0.5, verbose=False):\n",
    "    num_blocks = len(x0)\n",
    "    \n",
    "    delta = 0\n",
    "    for i in range(num_blocks):\n",
    "        delta = delta + np.sum(dx[i] * df0[i])\n",
    "    delta = alpha * delta\n",
    "    \n",
    "    f0 = f(x0)\n",
    "    \n",
    "    t = 1\n",
    "    x = update_blocks(x0, dx, t)\n",
    "    fx = f(x)\n",
    "    while (not np.isfinite(fx)) or f0+t*delta<fx:\n",
    "        t = beta*t\n",
    "        x = update_blocks(x0, dx, t)\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        print((t, delta))\n",
    "        l=-1e-5\n",
    "        u=1e-5\n",
    "        s = np.linspace(l, u, 64)\n",
    "        fs = np.zeros(s.size)\n",
    "        crit = f0 + s*delta\n",
    "        tan = f0 + s*delta/alpha\n",
    "        for i in range(s.size):\n",
    "            fs[i] = f(update_blocks(x0, dx, s[i]))\n",
    "        plt.plot(s, fs)\n",
    "        plt.plot(s, crit, '--')\n",
    "        plt.plot(s, tan, '.')\n",
    "        plt.scatter([0], [f0])\n",
    "        plt.show()\n",
    "            \n",
    "    return x, fx\n",
    "\n",
    "def negate_blocks(x):\n",
    "    # Helper function for negating the gradient of block variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = -x[i]\n",
    "    return z\n",
    "\n",
    "def block_norm(x):\n",
    "    num_blocks=len(x)\n",
    "    z = 0\n",
    "    for i in range(num_blocks):\n",
    "        z = z + np.sum(x[i]**2)\n",
    "    return np.sqrt(z)\n",
    "\n",
    "def random_matrix(shape, sigma=0.1):\n",
    "    # Helper for random initialization\n",
    "    return np.reshape(sigma*rd.randn(shape[0]*shape[1]), shape)\n",
    "\n",
    "### Begin gradient descent example\n",
    "\n",
    "### Random seed\n",
    "rd.seed(1234)\n",
    "\n",
    "data = load_breast_cancer() # Loads the Wisconsin Breast Cancer dataset (569 examples in 30 dimensions)\n",
    "\n",
    "# Parameters for the data\n",
    "dim_data = 30\n",
    "num_labels = 2\n",
    "num_examples = 569\n",
    "\n",
    "# Parameters for training\n",
    "num_train = 400\n",
    "\n",
    "X = data['data'] # Data in rows\n",
    "targets = data.target # 0-1 labels\n",
    "labels = np.zeros((num_examples, num_labels))\n",
    "for i in range(num_examples):\n",
    "    labels[i,targets[i]]=1 # Conversion to one-hot representations\n",
    "\n",
    "# Prepare hyperparameters of the network\n",
    "hidden_nodes = 20\n",
    "\n",
    "# Initialize variables\n",
    "W1_init = random_matrix((dim_data, hidden_nodes))\n",
    "b1_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W2_init = random_matrix((hidden_nodes, hidden_nodes))\n",
    "b2_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W3_init = random_matrix((hidden_nodes, num_labels))\n",
    "b3_init = np.zeros((1, num_labels))\n",
    "\n",
    "x = [W1_init, b1_init, W2_init, b2_init, W3_init, b3_init]\n",
    "f = nn_loss_closure(X[:num_train,:], labels[:num_train,:])\n",
    "df = nn_loss_gradient_closure(X[:num_train,:], labels[:num_train,:])\n",
    "dx = lambda v: negate_blocks(df(v))\n",
    "    \n",
    "for i in range(100):\n",
    "    ngrad = dx(x)\n",
    "    x, fval = block_backtracking(x, f, ngrad, df(x), alpha=0.1, verbose=False)\n",
    "    \n",
    "    train_data = matrix_softmax(logit(logit(X[:num_train,:]@x[0] + x[1]) @ x[2] + x[3]) @ x[4] + x[5])\n",
    "    train_labels = np.argmax(train_data, axis=1)\n",
    "    per_correct = 100*(1 - np.count_nonzero(train_labels - targets[:num_train])/num_train)\n",
    "\n",
    "    print(\"Step: %d, Avg Cross Entropy: %f, Gradient Norm: %f, Training Accuracy: %.1f percent\" % (i,fval,block_norm(ngrad), per_correct))\n",
    "    \n",
    "test_data = matrix_softmax(logit(logit(X[num_train:,:]@x[0] + x[1]) @ x[2] + x[3]) @ x[4] + x[5])\n",
    "test_labels = np.argmax(test_data, axis=1)\n",
    "per_correct = 100*(1 - np.count_nonzero(test_labels - targets[num_train:])/(num_examples-num_train))\n",
    "\n",
    "print('Final test accuracy: %.1f percent' % per_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "1. Less steps of backtracking. From step 5, training accuracy keeps unchanged.  \n",
    "2. Backtracking layer by layer.  \n",
    "3. Smaller training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.Variable([[1,0,1],[0,1,0],[1,0,1]], name = 'A')\n",
    "B = tf.Variable([[[[0,1,0],[1,0,1],[1,1,1]],[[0,1,0],[1,0,1],[0,1,0]]],\n",
    "                [[[0,1,0],[1,0,1],[0,1,0]],[[0,1,0],[1,0,1],[1,1,1]]]], name = 'B')\n",
    "f = tf.tensordot(A, B, [[0,1],[2,3]])\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}a_{11}&&a_{12}&&a_{13}\\\\a_{21}&&a_{22}&&a_{23}\\\\a_{31}&&a_{32}&&a_{33}\\end{pmatrix}*\\begin{pmatrix}1&&0\\\\0&&1\\end{pmatrix}\n",
    "$$\n",
    "can be written as\n",
    "$$\n",
    "\\text{Contract }\\mathcal{A}=\\begin{pmatrix}a_{11}&&a_{12}&&a_{13}\\\\a_{21}&&a_{22}&&a_{23}\\\\a_{31}&&a_{32}&&a_{33}\\end{pmatrix}\n",
    "$$\n",
    "$$\\text{ and }\\mathcal{B}=\\begin{pmatrix}\\begin{pmatrix}\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}&&\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\0&&1&&0\\end{pmatrix}\\end{pmatrix}\\begin{pmatrix}\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\0&&1&&0\\end{pmatrix}&&\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}\\end{pmatrix}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\text{ tensors over indicies }\\bf{i}=\\{1,2\\}\\text{ and }\\bf{j}=\\{3,4\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[2 0]\n",
      "   [0 2]]\n",
      "\n",
      "  [[3 3]\n",
      "   [3 3]]]\n",
      "\n",
      "\n",
      " [[[6 4]\n",
      "   [4 6]]\n",
      "\n",
      "  [[7 7]\n",
      "   [7 7]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.Variable([[[1,0,1],[0,1,0],[1,0,1]],[[1,1,1],[1,0,1],[1,1,1]]], name = 'A')\n",
    "B = tf.Variable([[[[[0,1,0],[1,0,1],[1,1,1]],[[0,1,0],[1,0,1],[0,1,0]]],\n",
    "                  [[[0,1,0],[1,0,1],[0,1,0]],[[0,1,0],[1,0,1],[1,1,1]]]],\n",
    "                 [[[[1,1,0],[1,0,1],[1,1,1]],[[1,1,0],[1,0,1],[1,1,1]]],\n",
    "                  [[[1,1,0],[1,0,1],[1,1,1]],[[1,1,0],[1,0,1],[1,1,1]]]]], name = 'B')\n",
    "f = tf.tensordot(A, B, [[1,2],[3,4]])\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Contract }\\mathcal{X}=\\begin{pmatrix}\\begin{pmatrix}a_{11}&&a_{12}&&a_{13}\\\\a_{21}&&a_{22}&&a_{23}\\\\a_{31}&&a_{32}&&a_{33}\\end{pmatrix}&&\\begin{pmatrix}b_{11}&&b_{12}&&b_{13}\\\\b_{21}&&b_{22}&&b_{23}\\\\b_{31}&&b_{32}&&b_{33}\\end{pmatrix}\\end{pmatrix}\\text{ and }\\mathcal{C}=\n",
    "$$\n",
    "$$\\begin{pmatrix}\\begin{pmatrix}\\begin{pmatrix}\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}&&\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\0&&1&&0\\end{pmatrix}\\end{pmatrix}\\begin{pmatrix}\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\0&&1&&0\\end{pmatrix}&&\\begin{pmatrix}0&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}\\end{pmatrix}\\end{pmatrix}\\\\ \\begin{pmatrix}\\begin{pmatrix}\\begin{pmatrix}1&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}&&\\begin{pmatrix}1&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}\\end{pmatrix}\\begin{pmatrix}\\begin{pmatrix}1&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}&&\\begin{pmatrix}1&&1&&0\\\\1&&0&&1\\\\1&&1&&1\\end{pmatrix}\\end{pmatrix}\\end{pmatrix}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\text{ tensors over indicies }\\bf{i}=\\{1,2\\}\\text{ and }\\bf{j}=\\{3,4\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Outline  \n",
    "Introduction:  \n",
    "1.\tWhat is K-means++?  \n",
    "2.\tProblems with traditional K-means.  \n",
    "3.\tComparing with other K-means methods, advantages, why chose this one?  \n",
    "4.\tHow does it apply or fit for our data, what to expect from the results.  \n",
    "  \n",
    "Clean Data:  \n",
    "1.\tIntroduce the attributes and details about data.(Describe and summary part)  \n",
    "2.\tBasic analysis for attributes and plot some graphs (EDA)  \n",
    "3.\tDescribe the problems and clean the data.  \n",
    "  \n",
    "K-means++ Algorithms:  \n",
    "1.\tRandomly choose a center point from data  \n",
    "2.\tFor each data points, compute their distance to the center, and sum up for each center point.  \n",
    "3.\tRandomly choose another center points, using a weighted probability distribution to calculate a final center point.  \n",
    "4.\tRepeat step 2 & 3 until k center points.  \n",
    "5.\tContinue original K-means clustering.  \n",
    "  \n",
    "Normal K-means:  \n",
    "1.\tIntroduction to general K-means.  \n",
    "2.\tAlgorithms introduction(Steps comparing with K-means++)  \n",
    "3.\tShow the results  \n",
    "  \n",
    "Comparing two methods and results:  \n",
    "1.\tDifference.  \n",
    "2.\tConclusion about which one is better.  \n",
    "  \n",
    "Conclusion:  \n",
    "1.\tHow K-means++ improves clustering results?  \n",
    "2.\tWhat can we tell about the cluster results from the data  \n",
    "3.\tProblems and difficulties during the project.  \n",
    "4.\tMore possibilities?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
